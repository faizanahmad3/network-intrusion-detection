# -*- coding: utf-8 -*-
"""Network Intrusion Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UoN0rQY_D6s790SECMyc73defr8CIyDR
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, mean_absolute_error
import statsmodels.api as sm
from numba import jit, cuda
from timeit import default_timer as timer
from shapash.explainer.smart_explainer import SmartExplainer
import shap
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from lazypredict.Supervised import LazyClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# @jit(target_backend='cuda')
data = pd.read_csv("E:\python projects\leetcodepractice\IoT dataset\IoT Network Intrusion Dataset.csv")

print("original dataset shape", data.shape)
duplicate_rows = data[data.duplicated()]
# print(data.info())
# print(data.head())
# print(data.describe())
# print(data.isna().sum().any())
# print(data.duplicated().any())
# clean_data = data.drop_duplicates()
clean_data = data
print("dataset shape after removing duplicates", clean_data.shape)
# print(clean_data.columns)

# import pandas as pd

# # set the chunk size
# chunksize = 100000

# # create an iterator of DataFrame objects representing chunks of the data
# data_chunks = pd.read_csv('E:\python projects\leetcodepractice\IoT dataset\IoT Network Intrusion Dataset.csv', chunksize=chunksize)

# # create an empty DataFrame to store the accumulated correlation matrix
# corr_matrix = pd.DataFrame()

# # iterate over the data chunks and accumulate the correlation matrix
# for chunk in data_chunks:
#     corr_matrix = corr_matrix.add(chunk.corr(), fill_value=0)

# print(len(data_chunks))
# print(data_chunks.dtype)

# divide the accumulated correlation matrix by the total number of chunks to get the average correlation matrix
# corr_matrix = corr_matrix / (len(data_chunks))

# import matplotlib.pyplot as plt

# # plot the heatmap
# plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
# plt.colorbar()
# plt.show()



plt.hist(data, bins=20)
plt.xlabel('X-axis label')
plt.ylabel('Y-axis label')
plt.title('Histogram')
plt.show()

plt.figure(figsize=(10,10))
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True)
plt.title('Correlation Matrix')
plt.show()

corr_matrix = clean_data.corr()
plt.figure(figsize=(10,10))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.colorbar()
plt.show()

# @jit(target_backend='cuda')
def detecting_zeros(clean_data = clean_data):
    num_zeros = {}
    for x in clean_data:
        column = clean_data[x]
        counts = column.value_counts()
        if 0 in counts.index:
            zeros = counts[0]
            num_zeros[x] = zeros
        else:
            zeros = 0
            num_zeros[x] = zeros
            # num_zeros = 0
    return pd.DataFrame.from_dict(num_zeros, orient="index")

# @jit(target_backend='cuda')
detected_zeros = detecting_zeros(clean_data)
# plt.figure(figsize=(10,10))
# sns.heatmap(detected_zeros)
# plt.show()

percentage_of_zeros = detected_zeros / clean_data.shape[0] * 100
print(percentage_of_zeros)

detecting_unnecessory_features = (percentage_of_zeros[percentage_of_zeros.values >= 80].index).tolist()
print(detecting_unnecessory_features)

clean_data.drop(columns= detecting_unnecessory_features, inplace=True)
# after_dropping_features_df = clean_data.drop(columns= detecting_unnecessory_features)
# unnecessory_features = ["Fwd_Pkt_Len_Max","Fwd_Pkt_Len_Min", "Fwd_Pkt_Len_Mean","Bwd_Pkt_Len_Max","Bwd_Pkt_Len_Min","Bwd_Pkt_Len_Mean"]
# dropping_features_manually = after_dropping_features_df.drop(columns=unnecessory_features)
print(clean_data.shape)

# plt.figure(figsize=(10,10))
# sns.heatmap(detected_zeros)
# plt.show()

# detected_zeros = detecting_zeros(clean_data)

# percentage_of_zeros = detected_zeros / clean_data.shape[0] * 100
# print(percentage_of_zeros)
# print(clean_data.shape)

# plt.figure(figsize=(10,10))
# sns.heatmap(detected_zeros)
# plt.show()

# @jit(target_backend='cuda')
order_label = {"Anomaly":1, "Normal":0}
clean_data['Label'] = clean_data['Label'].map(order_label)
le = LabelEncoder()
# ohe = OneHotEncoder()
# a = ohe.fit_transform(categorical['Label'])
clean_data[['Cat']] = clean_data[['Cat']].apply(le.fit_transform)
detecting_unnecessory_features = ['Sub_Cat','Flow_ID', 'Src_IP', 'Dst_IP', 'Timestamp', 'Init_Bwd_Win_Byts', 'Init_Fwd_Win_Byts']
clean_data.drop(columns= detecting_unnecessory_features, axis=1, inplace=True)
# clean_data.select_dtypes(include=["float64"])
print(clean_data.info())

# Select columns with 'float64' dtype
# float64_cols = list(clean_data.select_dtypes(include='float32'))

# The same code again calling the columns
# clean_data[float64_cols] = clean_data[float64_cols].astype('float64')
# print(clean_data.head(20))

# np.all(np.isfinite(mat))

# numerical = clean_data.select_dtypes(exclude=["object"])
# # print(numerical.head())
# categorical = clean_data.select_dtypes(include=["object"])
# print(numerical.columns)
# print(categorical.columns)

# final_data = pd.concat([numerical, categorical_label_encoder], axis=1)

X = clean_data.drop(["Cat"], axis=1)
y = clean_data.Cat

plt.figure(figsize=(15,15))
corr_matrix = clean_data.corr()
sns.heatmap(corr_matrix, annot=True)
plt.title('Correlation Matrix')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(np.clip(X,-1, (2*32)-1), y, test_size=0.20, random_state=42)

neigh = KNeighborsClassifier()
neigh.fit(X_train, y_train)

y_pred = neigh.predict(X_test)
print ("Accuracy for decision tree classifier : ",accuracy_score(y_test,y_pred))
print("Report : ",classification_report(y_test, y_pred))

from sklearn.svm import SVC
svc_model = SVC(kernel='rbf', random_state = 1)
svc_model.fit(X_train, y_train)

y_pred = svc_model.predict(X_test)
print ("Accuracy for decision tree classifier : ",accuracy_score(y_test,y_pred))
print("Report : ",classification_report(y_test, y_pred))

Cs = [0.001, 0.1, 1, 10, 100]
log_reg = LogisticRegression()
best_fit = GridSearchCV(estimator = log_reg, param_grid = dict(C=Cs) , cv = 5, scoring= 'accuracy')
best_fit.fit(X_train,y_train)
print("Best score is", best_fit.best_score_, 'for C =', float(best_fit.best_params_['C']))

log_reg = LogisticRegression(C=best_fit.best_params_['C'])
log_reg.fit(X_train,y_train)
predicted = log_reg.predict(X_test)
accuracy = accuracy_score(predicted, y_test)
print('Accuracy score: ', accuracy)

rfc=RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [4,5,6,7,8],
    'criterion' :['gini', 'entropy']
}
CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)
CV_rfc.fit(X_train, y_train)

CV_rfc.best_params_

rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=10, criterion='entropy')

rfc1.fit(X_train, y_train)

pred=rfc1.predict(X_test)
print("Accuracy for Random Forest on CV data: ",accuracy_score(y_test,pred))

clf_entropy = DecisionTreeClassifier(criterion = "entropy", random_state = 100, max_depth=10, min_samples_leaf=5)

    # Performing training
clf_entropy.fit(X_train, y_train)

y_pred = clf_entropy.predict(X_test)
print ("Accuracy for decision tree classifier : ",accuracy_score(y_test,y_pred))
print("Report : ",classification_report(y_test, y_pred))

clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)
models,predictions = clf.fit(X_train, X_test, y_train, y_test)

print(models)

# regressor.fit(X_train,y_train)
# print(accuracy_score(regressor.predict(X_test), y_test))
# print(f"mean absolute error  {mean_absolute_error(y_test, regressor.predict(X_test))}")
# print(f"train accuracy  {regressor.score(X_train, y_train)}")
# print(f"test accuracy  {regressor.score(X_test, y_test)}")
# print("random forest done")

# def training_without_gpu(X, y):
#     X_train, X_test, y_train, y_test = train_test_split(np.clip(X,-1, (2*32)-1), y, test_size=0.20, random_state=42)
#     regressor = RandomForestRegressor(n_estimators=100)
#     print("random forrest running")
#     regressor.fit(X_train,y_train)
#     # print(accuracy_score(regressor.predict(X_test), y_test))
#     y_pred = regressor.predict(X_test)
#     print(f"mean absolute error  {mean_absolute_error(y_test, y_pred)}")
#     # print(f"train accuracy  {regressor.score(X_train, y_train)}")
#     # print(f"test accuracy  {regressor.score(X_test, y_test)}")
#     print("random forest done")
#     return regressor

# # @jit(target_backend='cuda')
# def training_with_gpu(X, y):
#     X_train, X_test, y_train, y_test = train_test_split(np.clip(X,-1, (2*32)-1), y, test_size=0.20, random_state=42)
#     regressor = RandomForestRegressor(n_estimators=100)
#     print("random forrest running")
#     regressor.fit(X_train,y_train)
#     # print(accuracy_score(regressor.predict(X_test), y_test))
#     y_pred = regressor.predict(X_test)
#     print(f"mean absolute error  {mean_absolute_error(y_test, y_pred)}")
#     # print(f"train accuracy  {regressor.score(X_train, y_train)}")
#     # print(f"test accuracy  {regressor.score(X_test, y_test)}")
#     print("random forest done")
#     return regressor
# start = timer()
# regressor = training_without_gpu(X, y)
# print("without GPU:", timer()-start)

# start = timer()
# regressor = training_with_gpu(X, y)
# print("with GPU:", (timer()-start)/60, " minutes")

# @jit(target_backend='cuda')
# print("shap initjs")
# shap.initjs()
# print("tree explainer")
# explainer = shap.TreeExplainer(model=regressor)
# print("shap_values")
# shap_values = explainer.shap_values(X_test)
# print("predictor")
# predictor = shap_values.to_smartpredictor()
# print("predictor save")
# predictor.save('./predictor.pkl')

# xpl = SmartExplainer(model=regressor)
# xpl.compile(x=X_test)
# app = xpl.run_app(title_story='Tips Dataset')
# predictor = xpl.to_smartpredictor()
# predictor.save('./predictor.pkl')



