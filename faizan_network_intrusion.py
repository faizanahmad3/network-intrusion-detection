# -*- coding: utf-8 -*-
"""Noman Network Intrusion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M3ci21-78dXNY80mbP8kDeIAmDUeTA_H
"""

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, mean_absolute_error
from numba import jit, cuda
from timeit import default_timer as timer
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report

# @jit(target_backend='cuda')
data = pd.read_csv("E:\python projects\leetcodepractice\IoT dataset\IoT Network Intrusion Dataset.csv")

print("original dataset shape", data.shape)
# duplicate_rows = data[data.duplicated()]
print(data.info())
print(data.head()) # showing top 5 rows of 86 columns
print(data.describe())
print(data.isna().sum().any())
# print(data.duplicated().any())
# clean_data = data.drop_duplicates()
clean_data = data
# print("dataset shape after removing duplicates", clean_data.shape)
print(clean_data.shape)

plt.figure(figsize=(10,10))
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True)
plt.title('Correlation Matrix')
plt.show()

corr_matrix = clean_data.corr()
plt.figure(figsize=(10,10))
plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')
plt.colorbar()
plt.show()

# @jit(target_backend='cuda')
def detecting_zeros(clean_data = clean_data):
    num_zeros = {}
    for x in clean_data:
        column = clean_data[x]
        counts = column.value_counts()
        if 0 in counts.index:
            zeros = counts[0]
            num_zeros[x] = zeros
        else:
            zeros = 0
            num_zeros[x] = zeros
    return pd.DataFrame.from_dict(num_zeros, orient="index")

# @jit(target_backend='cuda')
detected_zeros = detecting_zeros(clean_data)
plt.figure(figsize=(10,10))
sns.heatmap(detected_zeros)
plt.show()

percentage_of_zeros = detected_zeros / clean_data.shape[0] * 100
print(f"percentage of zeros in every feature : {percentage_of_zeros}")

detecting_unnecessory_features = (percentage_of_zeros[percentage_of_zeros.values >= 80].index).tolist()
print(f"unnecessory features :{detecting_unnecessory_features}")

clean_data.drop(columns= detecting_unnecessory_features, inplace=True)
print(f"shape of data after removing unnecessory features : {clean_data.shape}")

detected_zeros = detecting_zeros(clean_data)

percentage_of_zeros = detected_zeros / clean_data.shape[0] * 100

plt.figure(figsize=(10,10))
sns.heatmap(detected_zeros)
plt.show()

# @jit(target_backend='cuda')
order_label = {"Anomaly":1, "Normal":0}
clean_data['Label'] = clean_data['Label'].map(order_label)

le = LabelEncoder()
clean_data[['Cat']] = clean_data[['Cat']].apply(le.fit_transform)

detecting_unnecessory_features = ['Sub_Cat','Flow_ID', 'Src_IP', 'Dst_IP', 'Timestamp', 'Init_Bwd_Win_Byts', 'Init_Fwd_Win_Byts']
clean_data.drop(columns= detecting_unnecessory_features, axis=1, inplace=True)

# print(clean_data.info())
X = clean_data.drop(["Cat"], axis=1)
y = clean_data.Cat

plt.figure(figsize=(15,15))
corr_matrix = clean_data.corr()
sns.heatmap(corr_matrix, annot=True)
plt.title('Correlation Matrix')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(np.clip(X,-1, (2*32)-1), y, test_size=0.20, random_state=42)



Cs = [0.001, 0.1, 1, 10, 100]
log_reg = LogisticRegression()
best_fit = GridSearchCV(estimator = log_reg, param_grid = dict(C=Cs) , cv = 5, scoring= 'accuracy')
best_fit.fit(X_train,y_train)
print("Best score is", best_fit.best_score_, 'for C =', float(best_fit.best_params_['C']))

log_reg = LogisticRegression(C=best_fit.best_params_['C'])
# log_reg = LogisticRegression(C=0.1)
log_reg.fit(X_train,y_train)

predicted = log_reg.predict(X_test)
accuracy = accuracy_score(y_test, predicted)
print('Accuracy score: ', accuracy)
print("Report : ",classification_report(y_test, predicted))



rfc=RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [4,5,6,7,8],
    'criterion' :['gini', 'entropy']
}
CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)
CV_rfc.fit(X_train, y_train)

CV_rfc.best_params_

rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=10, criterion='entropy')

rfc1.fit(X_train, y_train)

pred=rfc1.predict(X_test)
print("Accuracy for Random Forest on CV data: ",accuracy_score(y_test,pred))
print("Report : ",classification_report(y_test, pred))







